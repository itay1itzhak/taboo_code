{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "63538cfa76291845"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model_name = \"allenai/OLMo-2-1124-7B-Instruct\" #\"allenai/OLMo-7B\" #\"meta-llama/Llama-3.1-8B\"\n",
    "device = 'mps'\n",
    "\n",
    "\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    # torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ").to(device)"
   ],
   "id": "4202417c639e6b0e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load model directly\n",
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\"allenai/OLMo-7B-0724-hf\", trust_remote_code=True)"
   ],
   "id": "aaea9e09bb239549"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-7B-0724-hf\", trust_remote_code=True)"
   ],
   "id": "720870d5c72c2151"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "bpe_model = tokenizer.backend_tokenizer.model\n",
    "if hasattr(bpe_model, 'merges'):\n",
    "    merges = bpe_model.merges  # This is likely a list of merge pairs in order.\n",
    "elif hasattr(bpe_model, 'get_merges'):\n",
    "    merges = bpe_model.get_merges()"
   ],
   "id": "e102fdaa22252fb7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Access the underlying BPE model from the backend tokenizer\n",
    "bpe_model = tokenizer.backend_tokenizer.model\n",
    "\n",
    "# Try to access the merges attribute directly\n",
    "merges = bpe_model.merges\n",
    "print(merges)"
   ],
   "id": "23e60df33f14c9ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Get the underlying BPE model\n",
    "bpe_model = tokenizer.backend_tokenizer.model\n",
    "\n",
    "# Try to access the private attribute\n",
    "merges = getattr(bpe_model, \"_merges\", None)\n",
    "if merges is not None:\n",
    "    print(\"Merges (private):\", merges)\n",
    "else:\n",
    "    print(\"No _merges attribute found.\")"
   ],
   "id": "f661321edf7786a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "sorted(tokenizer.get_vocab().items(), key=lambda x: x[1], reverse=False)[:100]",
   "id": "3e7fcdb657a8ac09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# import os\n",
    "# from transformers import AutoTokenizer\n",
    "# from huggingface_hub import hf_hub_download\n",
    "#\n",
    "# # Load your tokenizer (make sure trust_remote_code is True if needed)\n",
    "# model_name = \"allenai/OLMo-7B-0724-hf\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "#\n",
    "# # -----------------------------------------------------------------------------\n",
    "# # Step 1: Locate the merges file using hf_hub_download\n",
    "# # -----------------------------------------------------------------------------\n",
    "# # The tokenizer object has a mapping of file names in its vocab_files_names.\n",
    "# vocab_files = tokenizer.vocab_files_names\n",
    "# print(\"Tokenizer file names:\", vocab_files)\n",
    "#\n",
    "# # Get the merges file name from the mapping\n",
    "# merges_filename = vocab_files.get(\"merges_file\")\n",
    "# if not merges_filename:\n",
    "#     raise ValueError(\"merges_file not found in tokenizer.vocab_files_names.\")\n",
    "#\n",
    "# # Use hf_hub_download to get the local path of the merges file.\n",
    "# # This downloads the file from the model repository (or retrieves it from cache)\n",
    "# merges_file_path = hf_hub_download(repo_id=model_name, filename=merges_filename)\n",
    "# print(\"Merges file path:\", merges_file_path)\n",
    "#\n",
    "# # -----------------------------------------------------------------------------\n",
    "# # Step 2: Read and parse the merges file to obtain merge rules\n",
    "# # -----------------------------------------------------------------------------\n",
    "# def load_merges(file_path):\n",
    "#     merge_rules = []\n",
    "#     with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         for line in f:\n",
    "#             line = line.strip()\n",
    "#             # Skip header/comment lines (often start with \"#\")\n",
    "#             if line.startswith(\"#\") or not line:\n",
    "#                 continue\n",
    "#             # Expect each non-comment line to have two tokens separated by whitespace\n",
    "#             parts = line.split()\n",
    "#             if len(parts) != 2:\n",
    "#                 continue  # or handle error if necessary\n",
    "#             merge_rules.append(tuple(parts))\n",
    "#     return merge_rules\n",
    "import json\n",
    "\n",
    "# Replace 'X' with the actual path to your JSON file\n",
    "json_path = '/Users/guykaplan/Dev/OLMo/test_fixtures/test-olmo-model/tokenizer.json'\n",
    "\n",
    "# Open and parse the JSON file\n",
    "try:\n",
    "    with open(json_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "        print(\"Parsed JSON data:\", data)\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found at path: {json_path}\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error decoding JSON: {e}\")\n",
    "merge_rules = data['model']['merges']\n",
    "print(f\"Loaded {len(merge_rules)} merge rules.\")\n",
    "\n",
    "# Create a dictionary mapping each merge pair to its rank (order of appearance).\n",
    "# Lower rank means the merge was applied earlier.\n",
    "bpe_ranks = {pair: rank for rank, pair in enumerate(merge_rules)}\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Define a function to decompose a token using the merge rules\n",
    "# -------------------------------\n",
    "def decompose_token(token, bpe_ranks):\n",
    "    \"\"\"\n",
    "    Decompose a token by simulating the reverse of BPE merge operations.\n",
    "    Start by splitting the token into characters, then repeatedly merge adjacent\n",
    "    characters if their join (separated by a space) is in the bpe_ranks.\n",
    "\n",
    "    Returns:\n",
    "      - components: The final list of sub-components obtained.\n",
    "      - merge_count: The number of merge operations applied.\n",
    "    \"\"\"\n",
    "    # Split the token into individual characters.\n",
    "    components = list(token)\n",
    "    merge_count = 0\n",
    "\n",
    "    while True:\n",
    "        candidate_index = None\n",
    "        candidate_rank = None\n",
    "        # Look at each adjacent pair in the current list of components.\n",
    "        for i in range(len(components) - 1):\n",
    "            # Form a string key that matches the format in bpe_ranks\n",
    "            pair_str = f\"{components[i]} {components[i+1]}\"\n",
    "            if pair_str in bpe_ranks:\n",
    "                rank = bpe_ranks[pair_str]\n",
    "                # Choose the candidate with the lowest rank (i.e. performed earlier in training)\n",
    "                if candidate_rank is None or rank < candidate_rank:\n",
    "                    candidate_rank = rank\n",
    "                    candidate_index = i\n",
    "        # If no merge candidate is found, exit the loop.\n",
    "        if candidate_index is None:\n",
    "            break\n",
    "        # Merge the chosen pair.\n",
    "        merge_count += 1\n",
    "        new_component = components[candidate_index] + components[candidate_index+1]\n",
    "        # Replace the two components with the merged version.\n",
    "        components = components[:candidate_index] + [new_component] + components[candidate_index+2:]\n",
    "    return components, merge_count\n",
    "\n",
    "# -------------------------------\n",
    "# Step 4: Compute and rank tokens by merge depth\n",
    "# -------------------------------\n",
    "vocab = tokenizer.get_vocab()  # dict: token (str) -> token_id (int)\n",
    "token_merge_depth = {}\n",
    "token_is_leaf = {}\n",
    "\n",
    "for token in vocab.keys():\n",
    "    _, merge_depth = decompose_token(token, bpe_ranks)\n",
    "    token_merge_depth[token] = merge_depth\n",
    "    token_is_leaf[token] = (merge_depth == 0)\n",
    "\n",
    "# Sort tokens by merge depth (higher merge count means more composite)\n",
    "sorted_tokens = sorted(token_merge_depth.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top 20 tokens by merge depth:\")\n",
    "for token, depth in sorted_tokens[:20]:\n",
    "    status = \"Leaf\" if token_is_leaf[token] else \"Non-leaf\"\n",
    "    print(f\"Token: {token:20s} | Merge Depth: {depth:3d} | {status}\")"
   ],
   "id": "1581f93d1358219e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_top_leaf_tokens(tokenizer, token_is_leaf, x):\n",
    "    \"\"\"\n",
    "    Returns a list of x tokens that are leaf tokens (i.e. non-composite, with merge depth 0)\n",
    "    and have the highest token IDs.\n",
    "\n",
    "    Parameters:\n",
    "      tokenizer: A Hugging Face tokenizer object with a get_vocab() method.\n",
    "      token_is_leaf: A dictionary mapping token strings to a boolean value (True if the token is a leaf).\n",
    "      x: The number of tokens to return.\n",
    "\n",
    "    Returns:\n",
    "      A list of x tokens (strings) that are leaves and have the highest token IDs.\n",
    "    \"\"\"\n",
    "    # Get the complete vocabulary (token -> token_id)\n",
    "    vocab = tokenizer.get_vocab()\n",
    "\n",
    "    # Filter tokens to include only leaves\n",
    "    leaf_tokens = [token for token, is_leaf in token_is_leaf.items() if is_leaf]\n",
    "\n",
    "    # Sort these leaf tokens by their token_id in descending order (highest token_id first)\n",
    "    sorted_leaf_tokens = sorted(leaf_tokens, key=lambda token: vocab[token], reverse=True)\n",
    "\n",
    "    # Return the top x tokens, or fewer if there are not enough.\n",
    "    return sorted_leaf_tokens[:x]\n",
    "\n",
    "# Example usage:\n",
    "# Assuming your tokenizer and token_is_leaf dictionary have been computed,\n",
    "# and you want the top 10 leaf tokens with the highest token IDs:\n",
    "top_leaf_tokens = get_top_leaf_tokens(tokenizer, token_is_leaf, 10000)\n",
    "print(\"Top 10 leaf tokens by highest token_id:\")\n",
    "for token in top_leaf_tokens:\n",
    "    print(f\"Token: {token:20s} | Token ID: {tokenizer.get_vocab()[token]}\")"
   ],
   "id": "c354fa571d44b983"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- custom list\n",
    "- characters\n",
    "- frequency criteria:\n",
    "    - X most frequent words in English\n",
    "    - X most frequent tokens in the tokenizer (is_full_word == True)\n",
    "    - X most frequent tokens in the dataset (WIMBD)\n",
    "- POS / synthetic criteria:\n",
    "    - verbs\n",
    "    - nouns\n",
    "    - adjectives\n",
    "    - numbers\n",
    "    - punctuation\n",
    "    - capitalization\n",
    "    - whitespace\n",
    "- semantic criteria:\n",
    "    - No abstract nouns\n",
    "    - No negation\n",
    "    - No\n",
    "\n"
   ],
   "id": "8065c5313b681f4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "token_is_leaf",
   "id": "900ea6650a187e4e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "sorted_tokens[-2000:-1800]",
   "id": "b768d8536f0c790f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Top 20 tokens by merge depth:\")\n",
    "for token, depth in sorted_tokens[-20:]:\n",
    "    print(f\"Token: {token:20s} | Merge Depth: {depth}\")"
   ],
   "id": "4aa550915cc9e0bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "sorted_tokens",
   "id": "2cd50ec0d80e4c25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "\n",
    "# Replace 'X' with the actual path to your JSON file\n",
    "json_path = '/Users/guykaplan/Dev/OLMo/test_fixtures/test-olmo-model/tokenizer.json'\n",
    "\n",
    "# Open and parse the JSON file\n",
    "try:\n",
    "    with open(json_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "        print(\"Parsed JSON data:\", data)\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found at path: {json_path}\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error decoding JSON: {e}\")"
   ],
   "id": "39a4ad557cab1660"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data['model']['merges'][:100]",
   "id": "903d3fb9ea4b9e83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "12ca4d03f9507a47"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
